# -*- coding: utf-8 -*-`r`nfrom __future__ import annotations
import math
import torch
import torch.nn.functional as F

def _ste_clamp(x: torch.Tensor, lo: float, hi: float) -> torch.Tensor:
    """Forward: clampÔºåBackward: identityÔºàÈÅøÂÖçÊ¢ØÂ∫¶Ë¢´Á°¨Êà™Êñ≠Ôºâ"""
    y = torch.clamp(x, min=lo, max=hi)
    return x + (y - x).detach()

def nll_iso3_e2(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
                logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:
    """
    Negative log-likelihood using pre-pooled squared error sum.
    e2sum: (B,T,1) or (B,T)
    logv : (B,T,1) or (B,T)
    mask : (B,T)
    """
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    if e2sum.dim() == 3 and e2sum.size(-1) == 1:
        e2sum = e2sum.squeeze(-1)
    lv = _ste_clamp(logv, logv_min, logv_max)
    # NaN-safe + ÊåâÊé©Á†ÅÊ∏ÖÔøΩ?
    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)
    e2sum = torch.where(mask.float() > 0.5, e2sum, torch.zeros_like(e2sum))
    v = torch.exp(lv).clamp_min(1e-12)
    nll = 0.5 * (3.0 * lv + e2sum / v)
    m = mask.float()
    return (nll * m).sum() / torch.clamp(m.sum(), min=1.0)

def nll_iso2_e2(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
                logv_min: float = -16.0, logv_max: float = 6.0) -> torch.Tensor:
    """Isotropic 2D negative log-likelihood for vision route."""
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    if e2sum.dim() == 3 and e2sum.size(-1) == 1:
        e2sum = e2sum.squeeze(-1)
    lv = _ste_clamp(logv, logv_min, logv_max)
    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)
    e2sum = torch.where(mask.float() > 0.5, e2sum, torch.zeros_like(e2sum))
    v = torch.exp(lv).clamp_min(1e-12)
    m = mask.float()
    nll = 0.5 * (2.0 * lv + e2sum / v)
    return (nll * m).sum() / torch.clamp(m.sum(), min=1.0)


def mse_anchor_1d(logv: torch.Tensor, y_var: torch.Tensor, mask: torch.Tensor, lam: float=1e-3) -> torch.Tensor:
    """
    Optional scale anchor on log-variance.
    y_var: (B,T) anchor variance (>=0), will be log() with clamp.
    """
    if logv.dim() == 3 and logv.size(-1) == 1:
        logv = logv.squeeze(-1)
    y = torch.clamp(y_var, min=1e-12).log()
    m = mask.float()
    se = (logv - y)**2 * m
    return lam * se.sum() / torch.clamp(m.sum(), min=1.0)

def nll_diag_axes(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,
                  logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:
    """
    ÂêÑÂêëÂºÇÊÄßÂØπËßíÈ´òÔøΩ?NLLÔºàÈÄêËΩ¥Ôºâ„ÄÇÈÄÇÁî®ÔøΩ?GNSS ENU ‰∏âËΩ¥ÔøΩ?
    e2_axes  : (B,T,3)   ÊØèËΩ¥ËØØÂ∑ÆÂπ≥Êñπ
    logv_axes: (B,T,3)   ÊØèËΩ¥ log(œÉ^2)
    mask_axes: (B,T,3)   ÊØèËΩ¥ÊúâÊïàÊé©Á†Å
    """
    lv = _ste_clamp(logv_axes, logv_min, logv_max)
    inv_v = torch.exp(-lv)                 # (B,T,3)
    nll = 0.5 * (e2_axes * inv_v + lv)    # (B,T,3)
    m = mask_axes.float()
    num = (nll * m).sum()
    den = torch.clamp(m.sum(), min=1.0)
    return num / den

def nll_diag_axes_weighted(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,
                           axis_w: torch.Tensor=None,
                           logv_min: float=-16.0, logv_max: float=6.0):
    """
    ÂêÑÂêëÂºÇÊÄßÂØπËßíÈ´òÔøΩ?NLLÔºàÈÄêËΩ¥ÔøΩ? ÊåâËΩ¥ÊùÉÈáçÔøΩ?
    e2_axes, logv_axes, mask_axes: (B,T,3)
    axis_w: (3,) ÂΩí‰∏ÄÂà∞ÂùáÔøΩ?1 Êõ¥Á®≥ÔºàÂ§ñÈÉ®ÂèØÂÖàÂÅöÂΩí‰∏ÄÂåñÔºâ
    """
    lv = _ste_clamp(logv_axes, logv_min, logv_max)
    e2_axes = torch.nan_to_num(e2_axes, nan=0.0, posinf=0.0, neginf=0.0)
    e2_axes = torch.where(mask_axes.float() > 0.5, e2_axes, torch.zeros_like(e2_axes))
    inv_v = torch.exp(-lv)                    # (B,T,3)
    nll_axes = 0.5 * (e2_axes * inv_v + lv)  # (B,T,3)
    m = mask_axes.float()
    num = nll_axes.mul(m).sum(dim=(0,1))      # (3,)
    den = m.sum(dim=(0,1)).clamp_min(1.0)     # (3,)
    per_axis = num / den                       # (3,)
    if axis_w is None:
        axis_w = torch.ones_like(per_axis)
    # ÂΩí‰∏ÄÂà∞ÂùáÔøΩ?1Ôºå‰æøÔøΩ?lr Á®≥ÂÆö
    axis_w = axis_w * (3.0 / axis_w.sum().clamp_min(1e-6))
    return (per_axis * axis_w).sum(), per_axis.detach()

def nll_studentt_diag_axes(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,
                           nu: float = 3.0, logv_min: float = -16.0, logv_max: float = 6.0):
    """
    ÂêÑÂêëÂºÇÊÄßÂØπÔøΩ?Student-t NLLÔºàÈÄêËΩ¥Ôºâ„ÄÇÂØπÂºÇÂ∏∏ÂÄºÊõ¥Á®≥ÂÅ•ÔøΩ?
    e2_axes  : (B,T,3)   ÊØèËΩ¥ËØØÂ∑ÆÂπ≥Êñπ
    logv_axes: (B,T,3)   ÊØèËΩ¥ log(œÉ^2)
    mask_axes: (B,T,3)   ÊØèËΩ¥ÊúâÊïàÊé©Á†Å
    nu       : Ëá™Áî±Â∫¶ÂèÇÊï∞ÔºàË∂äÂ∞èË∂äÈáçÂ∞æÔºåË∂äÁ®≥ÂÅ•Ôºâ
    """
    lv = _ste_clamp(logv_axes, logv_min, logv_max)
    e2_axes = torch.nan_to_num(e2_axes, nan=0.0, posinf=0.0, neginf=0.0)
    e2_axes = torch.where(mask_axes.float() > 0.5, e2_axes, torch.zeros_like(e2_axes))
    v  = torch.exp(lv).clamp_min(1e-12)
    m  = mask_axes.float()
    # Student-t NLLÔºàÁúÅÁï•Â∏∏Êï∞È°πÔºâÔºö0.5*log(v) + 0.5*(nu+1)*log(1 + e2/(nu*v))
    nll = 0.5*lv + 0.5*(nu + 1.0) * torch.log1p(e2_axes / (v * nu))
    num = (nll * m).sum()
    den = m.sum().clamp_min(1.0)
    return num / den

def mse_anchor_axes(logv_axes: torch.Tensor, y_var_axes: torch.Tensor, mask_axes: torch.Tensor, lam: float=1e-4) -> torch.Tensor:
    """
    GNSS ÈÄêËΩ¥ log-variance ÁöÑËΩØÈîöÔºöÊääÈ¢ÑÔøΩ?logv ËΩªÂæÆÊãâÂêë log(vendor^2)ÔøΩ?
    logv_axes   : (B,T,3)
    y_var_axes  : (B,T,3)  ‚ÄîÔøΩ?ÈÄêËΩ¥ vendor Êä•ÂëäÁöÑÊñπÂ∑ÆÔºà‰∏çÊòØÊ†áÂáÜÂ∑ÆÔºâ
    mask_axes   : (B,T,3)
    """
    lv = logv_axes
    y  = torch.clamp(y_var_axes, min=1e-12).log()
    m  = mask_axes.float()
    se = (lv - y)**2 * m
    return lam * se.sum() / torch.clamp(m.sum(), min=1.0)

def adaptive_nll_loss(logv: torch.Tensor, e2: torch.Tensor, mask: torch.Tensor, 
                      use_step_labels, y_anchor: torch.Tensor = None,
                      logv_min: float = -16.0, logv_max: float = 6.0,
                      route: str = "acc") -> torch.Tensor:
    """
    Ëá™ÈÄÇÂ∫îNLLÊçüÂ§±ÂáΩÊï∞ÔøΩ?
    - Â¶ÇÊûúÊúâÊ≠•Á∫ßÊ†áÁ≠æÔºå‰ΩøÁî®Á∫ØÊ≠•Á∫ßNLL
    - Âê¶Âàô‰ΩøÁî®Á™óÂè£ÈîöÁÇπ+ËΩª‰∏≠ÂøÉÂåñ
    
    Args:
        logv: (B,T,D) Ê®°ÂûãËæìÂá∫ÁöÑlogÊñπÂ∑Æ
        e2: (B,T,D) Ê≠•Á∫ßËØØÂ∑ÆÂπ≥Êñπ ÔøΩ?‰ªéÁ™óÂè£Ê†áÁ≠æÊâ©Â±ïÁöÑËØØÂ∑ÆÂπ≥Êñπ
        mask: (B,T) Êé©Á†Å
        use_step_labels: bool or tensor ÊòØÂê¶‰ΩøÁî®Ê≠•Á∫ßÊ†áÁ≠æ
        y_anchor: (B,D) Á™óÂè£Á∫ßÊ†áÁ≠æÔºà‰ªÖÂú®ÈùûÊ≠•Á∫ßÊ®°Âºè‰ΩøÁî®Ôºâ
        route: str Ë∑ØÁî±Á±ªÂûã ("acc", "gyr", "vis")
    """
    # Â§ÑÁêÜuse_step_labelsÂèØËÉΩÊòØtensorÁöÑÊÉÖÔøΩ?
    if isinstance(use_step_labels, torch.Tensor):
        use_step_labels = bool(use_step_labels.item())
    
    # --- ÊÉÖÂÜµ AÔºöÊúâÊ≠•Á∫ßÊ†áÁ≠æÔºàÊé®ËçêÔºâ ---
    if use_step_labels:
        if route in ("acc", "gyr"):
            # IMU: ‰ΩøÁî®3ËΩ¥ISOÊçüÂ§±
            if e2.size(-1) == 3:
                # ÂØπ‰∫é‰∏âËΩ¥Êï∞ÊçÆÔºåÊ±ÇÂíåÂêéÈô§‰ª•3
                e2sum = e2.sum(dim=-1)  # (B,T)
                if logv.size(-1) == 3:
                    # Â¶ÇÊûúÊ®°ÂûãËæìÂá∫‰πüÊòØ3ËΩ¥ÔºåÂèñÂπ≥ÔøΩ?
                    logv_avg = logv.mean(dim=-1)  # (B,T)
                else:
                    logv_avg = logv.squeeze(-1)  # (B,T)
                return nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)
            else:
                # ÂçïËΩ¥Ê®°Âºè
                e2sum = e2.squeeze(-1)  # (B,T)
                logv_avg = logv.squeeze(-1)  # (B,T)
                return nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)
        else:  # vis
            # ËßÜËßâÔºö‰ΩøÔøΩ?ËΩ¥ISOÊçüÂ§±
            e2sum = e2.squeeze(-1)  # (B,T)
            logv_avg = logv.squeeze(-1)  # (B,T)
            return nll_iso2_e2(e2sum, logv_avg, mask, logv_min, logv_max)
    
    # --- ÊÉÖÂÜµ BÔºöÂè™ÊúâÁ™óÂè£Ê†áÁ≠æÔºàÔøΩ?EUROC npz ÁöÑÊùÉÂÆú‰πãËÆ°Ôºâ ---
    else:
        # Âü∫Á°ÄÊ≠•Á∫ßNLLÔºà‰ΩøÁî®Êâ©Â±ïÁöÑe2ÔøΩ?
        e2sum = e2.squeeze(-1) if e2.size(-1) == 1 else e2.sum(dim=-1)
        logv_avg = logv.squeeze(-1) if logv.size(-1) == 1 else logv.mean(dim=-1)
        
        if route in ("acc", "gyr"):
            nll = nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)
        else:  # vis
            nll = nll_iso2_e2(e2sum, logv_avg, mask, logv_min, logv_max)
        
        # "‰∏≠ÂøÉÔøΩ?ÔºöËΩªÂæÆÁ∫¶ÔøΩ?E[z¬≤] -> 1ÔºåÈò≤Ê≠¢Êî∂ÊïõÂà∞Â∏∏Êï∞
        sig2 = torch.exp(logv_avg).clamp_min(1e-12)
        z2 = e2sum / sig2  # Ê†áÂáÜÂåñËØØÔøΩ?
        m = mask.float()
        z2_mean = (z2 * m).sum() / torch.clamp(m.sum(), min=1.0)
        loss_center = (z2_mean - 1.0).pow(2)
        
        # "Á™óÂè£Â∞∫Â∫¶ÔøΩ?ÔºöÁî®Êó∂Èó¥Âπ≥ÂùáÁöÑœÉ¬≤‰∏éY_anchorÂØπÈΩê
        if y_anchor is not None:
            sig2_win = sig2.mean(dim=1)  # (B,)
            if route in ("acc", "gyr") and y_anchor.size(-1) == 3:
                # ‰∏âËΩ¥Ê†áÁ≠æÊ±ÇÂπ≥ÔøΩ?
                y_target = y_anchor.mean(dim=-1)  # (B,)
            else:
                y_target = y_anchor.squeeze(-1)  # (B,)
            anchor_loss = F.smooth_l1_loss(sig2_win, y_target)
        else:
            anchor_loss = torch.tensor(0.0, device=logv.device)
        
        return nll + 1e-3 * loss_center + 1e-4 * anchor_loss

def nll_gauss_huber_iso3(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
                         logv_min: float=-12.0, logv_max: float=6.0,
                         delta: float=1.5,                # Huber ÈòàÔøΩ?
                         lam_center: float=5e-2,          # z¬≤ÂùáÂÄºÊ†°ÂáÜÊ≠£ÂàôÊùÉÔøΩ?
                         z2_target: float=1.0,            # ÁõÆÊ†á E[z¬≤]
                         y_anchor: torch.Tensor | None=None,   # (B,) ÔøΩ?(B,T,1/3)
                         anchor_weight: float=0.0,        # Á™óÂè£Â∞∫Â∫¶ÈîöÊùÉÔøΩ?
                         df: float=3.0) -> torch.Tensor:
    """
    Huber-NLLÊçüÂ§±ÂáΩÊï∞ÔºöÂØπ z = ÔøΩ?e¬≤/œÉ¬≤) ‰ΩøÁî®pseudo-HuberÔºå‰øùÁïôÂ∞∫Â∫¶Â≠¶‰π†ËÉΩÔøΩ?
    
    Args:
        e2sum: (B,T) ÔøΩ?(B,T,1) ËØØÂ∑ÆÂπ≥ÊñπÔøΩ?
        logv: (B,T) ÔøΩ?(B,T,1) logÊñπÂ∑Æ
        mask: (B,T) Êé©Á†Å
        delta: HuberÈòàÂÄºÔºå|z|‚â§Œ¥Êó∂‰∏∫‰∫åÊ¨°ÔºåÂê¶Âàô‰∏∫‰∏ÄÔøΩ?
        lam_center: z¬≤ÂùáÂÄºÊ†°ÂáÜÊ≠£ÂàôÊùÉÔøΩ?
        z2_target: ÁõÆÊ†áE[z¬≤]ÔøΩ?
        y_anchor: Á™óÂè£Â∞∫Â∫¶ÈîöÁÇπ
        anchor_weight: ÈîöÁÇπÊùÉÈáç
        df: Ëá™Áî±Â∫¶ÔºàIMU‰∏âËΩ¥ÂêàÂπ∂ÔøΩ?3ÔøΩ?
    
    Returns:
        loss: Ê†áÈáèÊçüÂ§±ÔøΩ?
    """
    # squeeze ÔøΩ?(B,T)
    if logv.dim() == 3 and logv.size(-1) == 1: 
        logv = logv.squeeze(-1)
    if e2sum.dim() == 3 and e2sum.size(-1) == 1: 
        e2sum = e2sum.squeeze(-1)
    
    m = mask.float()
    
    # üîß ÂÖ≥ÈîÆÔºöÊ∏ÖÔøΩ?+ Âè™Âú®ÊúâÊïàÊé©Á†ÅÂÜÖËÆ°ÂÖ•ËØØÔøΩ?
    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)
    e2sum = torch.where(m > 0.5, e2sum, torch.zeros_like(e2sum))
    
    # ÔøΩ?STE clampÔºåÈÅøÂÖçÊ¢ØÂ∫¶Ë¢´Á°¨Êà™ÔøΩ?
    lv = _ste_clamp(logv, logv_min, logv_max)
    v = torch.exp(lv).clamp_min(1e-12)

    # z ÔøΩ?pseudo-Huber œÅ(z)
    z2 = (e2sum / v).clamp_min(0.0)              # (B,T)
    z = torch.sqrt(z2 + 1e-12)
    rho = torch.where(z <= delta, 0.5*z2, delta*(z - 0.5*delta))

    # NLLÔºöÔøΩ?z) + 0.5 * df * logœÉ¬≤
    nll = (rho + 0.5*df*lv) * m
    den = m.sum().clamp_min(1.0)
    nll = nll.sum() / den

    # ËΩªÂ∫¶Ê†°ÂáÜÔºöÊää E[z¬≤] ÊãâÂõû 1ÔºàÁî®Âêå‰∏ÄÔøΩ?denÔøΩ?
    z2_mean = (z2 * m).sum() / den
    loss_center = (z2_mean - z2_target).pow(2)

    # ÔºàÂèØÈÄâÔºâÁ™óÂè£Â∞∫Â∫¶ÈîöÔºöÊó∂Èó¥Âπ≥Âùá œÉ¬≤ ‰∏éÂ§ñÈÉ®Èîö y_anchor ÂØπÈΩê
    if (y_anchor is not None) and (anchor_weight > 0.0):
        sig2_win = v.mean(dim=1)                        # (B,)
        if y_anchor.dim() == 3 and y_anchor.size(-1) == 3:
            y_target = y_anchor.mean(dim=(-1,-2))       # (B,T,3)->(B,)
        elif y_anchor.dim() == 2:
            y_target = y_anchor.mean(dim=1)             # (B,)
        else:
            y_target = y_anchor.squeeze(-1)             # (B,)
        anchor_loss = F.smooth_l1_loss(sig2_win, y_target)
    else:
        anchor_loss = torch.zeros((), device=logv.device)

    return nll + lam_center * loss_center + anchor_weight * anchor_loss

def nll_gauss_huber_iso2(
    e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,
    logv_min: float=-12.0, logv_max: float=6.0,
    delta: float=1.5,
    lam_center: float=0.0, z2_target: float=1.0,
    y_anchor: torch.Tensor | None=None, anchor_weight: float=0.0,
    df: float=2.0
):
    import torch
    if logv.dim() >= 2 and logv.size(-1) == 1: logv = logv.squeeze(-1)
    if e2sum.dim() >= 2 and e2sum.size(-1) == 1: e2sum = e2sum.squeeze(-1)
    lv = _ste_clamp(logv, logv_min, logv_max)
    v  = torch.exp(lv).clamp_min(1e-12)

    m = mask.float()
    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)
    e2sum = torch.where(m > 0.5, e2sum, torch.zeros_like(e2sum))

    z2 = (e2sum / v).clamp_min(0.0)
    z  = torch.sqrt(z2 + 1e-12)
    quad = 0.5 * z * z
    lin  = delta * (z - 0.5 * delta)
    rho  = torch.where(z <= delta, quad, lin)

    nll = (rho + 0.5 * df * lv) * m
    den = m.sum().clamp_min(1.0)
    nll = nll.sum() / den

    if lam_center > 0.0:
        z2_mean = (z2 * m).sum() / den
        nll = nll + lam_center * (z2_mean - z2_target).pow(2)

    if (anchor_weight > 0.0) and (y_anchor is not None):
        sig2 = v
        if sig2.dim() >= 2 and sig2.size(-1) == 1: sig2 = sig2.squeeze(-1)
        y = torch.clamp(y_anchor.squeeze(-1), min=1e-12)
        nll = nll + anchor_weight * torch.mean((sig2.log() - y.log())**2)

    return nll

# === VIS: step-wise e2 losses (df=2), with soft clamp ===
import torch
import torch.nn.functional as F

def _soft_clamp(x: torch.Tensor, lo: float, hi: float, beta: float = 4.0) -> torch.Tensor:
    # Forward ‰ªçÁÑ∂Ëøë‰ºº clampÔºå‰ΩÜ‰∏§Á´Ø‰øùÁïôÊ¢ØÂ∫¶ÔºõÈÅøÂÖç‰∏Ä‰∏äÊù•Ë¥¥ËæπÂØºËá¥‚ÄúÊó†Ê¢ØÂ∫¶ÔøΩ?
    return lo + F.softplus(x - lo, beta=beta) - F.softplus(x - hi, beta=beta)

def ema_filter_1d(x: torch.Tensor, tau: int = 5) -> torch.Tensor:
    """Apply EMA along time; supports [B,T] or [T]."""
    squeeze = False
    if x.dim() == 1:
        x = x.unsqueeze(0)
        squeeze = True
    if tau <= 1:
        return x.squeeze(0) if squeeze else x
    y = torch.zeros_like(x)
    alpha = 2.0 / (float(tau) + 1.0)
    y[:, 0] = x[:, 0]
    for t in range(1, x.size(1)):
        y[:, t] = alpha * x[:, t] + (1.0 - alpha) * y[:, t - 1]
    return y.squeeze(0) if squeeze else y


def huber_on_z(z: torch.Tensor, delta: float) -> torch.Tensor:
    absz = z.abs()
    quad = 0.5 * z * z
    lin = delta * (absz - 0.5 * delta)
    return torch.where(absz <= delta, quad, lin)

def nll_gaussian_e2_step(
    e2_step: torch.Tensor,    # (B,T) ÈÄêÊ≠•Âπ≥ÊñπËØØÂ∑ÆÔºàÂÉèÁ¥†^2ÔøΩ?
    logv_raw: torch.Tensor,   # (B,T) Ê®°ÂûãËæìÂá∫ÔøΩ?log-varianceÔºàÊú™Â§πÁ¥ßÔøΩ?
    mask_step: torch.Tensor,  # (B,T) ÈÄêÊ≠•Êé©Á†ÅÔøΩ?/1ÔøΩ?
    logv_min: float, logv_max: float, df: float = 2.0
):
    lv = _soft_clamp(logv_raw, logv_min, logv_max, beta=4.0)
    e2 = torch.clamp(torch.nan_to_num(e2_step, 0.0, 0.0, 0.0), min=0.0)
    m  = (mask_step > 0.5).float()
    nll = 0.5 * (e2 * torch.exp(-lv) + df * lv)   # ÈÄêÊ≠• NLL
    return (nll * m).sum() / (m.sum() + 1e-8), lv

def nll_gauss_huber_e2_step(
    e2_step: torch.Tensor, logv_raw: torch.Tensor, mask_step: torch.Tensor,
    logv_min: float, logv_max: float, delta: float = 1.5, df: float = 2.0
):
    lv = _soft_clamp(logv_raw, logv_min, logv_max, beta=4.0)
    m  = (mask_step > 0.5).float()
    e2 = torch.nan_to_num(e2_step, nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0.0)  # NaNÊ∏ÖÁêÜ‰∏éÈùûË¥üÊà™ÔøΩ?
    z2 = e2 * torch.exp(-lv)
    z  = torch.sqrt(z2 + 1e-12)
    rho = torch.where(z <= delta, 0.5*z*z, delta*(z - 0.5*delta))
    nll = rho + 0.5 * df * lv
    return (nll * m).sum() / (m.sum() + 1e-8), lv


def nll_gauss_huber_e2_step_with_ema(
    e2_step: torch.Tensor,
    logv_raw: torch.Tensor,
    mask_step: torch.Tensor,
    logv_min: float,
    logv_max: float,
    delta: float = 1.2,
    df: float = 2.0,
    lam_center: float = 5e-2,
    alpha_aux: float = 0.05,
    ema_tau: int = 5,
):
    lv = _soft_clamp(logv_raw, logv_min, logv_max, beta=4.0)
    e2 = torch.nan_to_num(e2_step, nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0.0)
    if mask_step is not None:
        m = (mask_step > 0.5).float()
        m_sum = m.sum().clamp_min(1.0)
    else:
        m = None
        m_sum = None
    v = torch.exp(lv).clamp_min(1e-12)
    z2 = (e2 / v).clamp_min(0.0)
    z = torch.sqrt(z2 + 1e-12)
    rho = huber_on_z(z, delta)
    nll = rho + 0.5 * df * lv
    if m is not None:
        nll_mean = (nll * m).sum() / m_sum
        z2_mean = (z2 * m).sum() / m_sum
    else:
        nll_mean = nll.mean()
        z2_mean = z2.mean()
    if lam_center > 0.0:
        center = lam_center * (z2_mean - df).pow(2)
    else:
        center = torch.zeros((), device=lv.device, dtype=lv.dtype)
    aux = torch.zeros((), device=lv.device, dtype=lv.dtype)
    if alpha_aux and alpha_aux > 0.0:
        ema_tau_int = max(1, int(ema_tau))
        e2_over_df = e2 / float(df if df != 0 else 1.0)
        ema = ema_filter_1d(e2_over_df, tau=ema_tau_int).clamp_min(1e-12).log()
        diff = (lv - ema).abs()
        if m is not None:
            aux = (diff * m).sum() / m_sum
        else:
            aux = diff.mean()
        aux = alpha_aux * aux
    loss = nll_mean + center + aux
    info = {
        "nll": nll_mean.detach(),
        "center": center.detach(),
        "aux": aux.detach(),
        "z2_mean": z2_mean.detach(),
    }
    return loss, info

def vis_center_regularization(e2_step, lv_step, mask_step, df: float = 2.0, lam_center: float = 5e-2):
    """Ensure VIS variance predictions stay calibrated by nudging E[z^2] toward `df`."""
    if lam_center <= 0.0:
        return torch.zeros((), device=lv_step.device, dtype=lv_step.dtype)
    if e2_step.dim() == 3 and e2_step.size(-1) == 1:
        e2_step = e2_step.squeeze(-1)
    if lv_step.dim() == 3 and lv_step.size(-1) == 1:
        lv_step = lv_step.squeeze(-1)
    m = mask_step.float()
    e2_step = torch.nan_to_num(e2_step, nan=0.0, posinf=0.0, neginf=0.0)
    e2_step = torch.where(m > 0.5, e2_step, torch.zeros_like(e2_step))
    v = torch.exp(lv_step).clamp_min(1e-12)
    z2 = (e2_step / v).clamp_min(0.0)
    den = m.sum().clamp_min(1.0)
    z2_mean = (z2 * m).sum() / den
    return lam_center * (z2_mean - df).pow(2)

def nll_vis_diag_2d(ex_step, ey_step, lvx, lvy, mask_step, logv_min: float = -12.0, logv_max: float = 6.0):
    """Diagonal 2D Gaussian NLL for VIS residuals."""
    lvx = _ste_clamp(lvx, logv_min, logv_max)
    lvy = _ste_clamp(lvy, logv_min, logv_max)
    m = mask_step.float()
    ex2 = torch.nan_to_num(ex_step, nan=0.0, posinf=0.0, neginf=0.0).pow(2)
    ey2 = torch.nan_to_num(ey_step, nan=0.0, posinf=0.0, neginf=0.0).pow(2)
    ex2 = torch.where(m > 0.5, ex2, torch.zeros_like(ex2))
    ey2 = torch.where(m > 0.5, ey2, torch.zeros_like(ey2))
    vx = torch.exp(lvx).clamp_min(1e-12)
    vy = torch.exp(lvy).clamp_min(1e-12)
    nll_x = 0.5 * (ex2 * torch.exp(-lvx) + lvx)
    nll_y = 0.5 * (ey2 * torch.exp(-lvy) + lvy)
    nll = (nll_x + nll_y) * m
    return nll.sum() / m.sum().clamp_min(1.0)

def vis_center_regularization_2d(ex_step, ey_step, lvx, lvy, mask_step, lam_center: float = 5e-2):
    """Center calibration for VIS diagonal outputs (target E[z^2]=1 per axis)."""
    if lam_center <= 0.0:
        return torch.zeros((), device=lvx.device, dtype=lvx.dtype)
    if ex_step.dim() == 3 and ex_step.size(-1) == 1:
        ex_step = ex_step.squeeze(-1)
    if ey_step.dim() == 3 and ey_step.size(-1) == 1:
        ey_step = ey_step.squeeze(-1)
    if lvx.dim() == 3 and lvx.size(-1) == 1:
        lvx = lvx.squeeze(-1)
    if lvy.dim() == 3 and lvy.size(-1) == 1:
        lvy = lvy.squeeze(-1)
    m = mask_step.float()
    ex2 = torch.nan_to_num(ex_step, nan=0.0, posinf=0.0, neginf=0.0).pow(2)
    ey2 = torch.nan_to_num(ey_step, nan=0.0, posinf=0.0, neginf=0.0).pow(2)
    ex2 = torch.where(m > 0.5, ex2, torch.zeros_like(ex2))
    ey2 = torch.where(m > 0.5, ey2, torch.zeros_like(ey2))
    vx = torch.exp(_ste_clamp(lvx, -16.0, 6.0)).clamp_min(1e-12)
    vy = torch.exp(_ste_clamp(lvy, -16.0, 6.0)).clamp_min(1e-12)
    z2x = (ex2 / vx).clamp_min(0.0)
    z2y = (ey2 / vy).clamp_min(0.0)
    den = m.sum().clamp_min(1.0)
    mean_x = (z2x * m).sum() / den
    mean_y = (z2y * m).sum() / den
    target = 1.0
    return lam_center * ((mean_x - target).pow(2) + (mean_y - target).pow(2))

