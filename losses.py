# -*- coding: utf-8 -*-
from __future__ import annotations

import math

import torch

import torch.nn.functional as F



def _ste_clamp(x: torch.Tensor, lo: float, hi: float) -> torch.Tensor:

    """Forward: clampï¼ŒBackward: identityï¼ˆé¿å…æ¢¯åº¦è¢«ç¡¬æˆªæ–­ï¼‰"""

    y = torch.clamp(x, min=lo, max=hi)

    return x + (y - x).detach()



def nll_iso3_e2(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,

                logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:

    """

    Negative log-likelihood using pre-pooled squared error sum.

    e2sum: (B,T,1) or (B,T)

    logv : (B,T,1) or (B,T)

    mask : (B,T)

    """

    if logv.dim() == 3 and logv.size(-1) == 1:

        logv = logv.squeeze(-1)

    if e2sum.dim() == 3 and e2sum.size(-1) == 1:

        e2sum = e2sum.squeeze(-1)

    lv = _ste_clamp(logv, logv_min, logv_max)

    # NaN-safe + æŒ‰æ©ç æ¸…ï¿½?

    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)

    e2sum = torch.where(mask.float() > 0.5, e2sum, torch.zeros_like(e2sum))

    v = torch.exp(lv).clamp_min(1e-12)

    nll = 0.5 * (3.0 * lv + e2sum / v)

    m = mask.float()

    return (nll * m).sum() / torch.clamp(m.sum(), min=1.0)








def mse_anchor_1d(logv: torch.Tensor, y_var: torch.Tensor, mask: torch.Tensor, lam: float=1e-3) -> torch.Tensor:

    """

    Optional scale anchor on log-variance.

    y_var: (B,T) anchor variance (>=0), will be log() with clamp.

    """

    if logv.dim() == 3 and logv.size(-1) == 1:

        logv = logv.squeeze(-1)

    y = torch.clamp(y_var, min=1e-12).log()

    m = mask.float()

    se = (logv - y)**2 * m

    return lam * se.sum() / torch.clamp(m.sum(), min=1.0)



def nll_diag_axes(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,

                  logv_min: float=-16.0, logv_max: float=6.0) -> torch.Tensor:

    """
    Anisotropic diagonal Gaussian NLL (per-axis).

    e2_axes  : (B,T,3)   per-axis error squared

    logv_axes: (B,T,3)   æ¯è½´ log(Ïƒ^2)

    mask_axes: (B,T,3)   æ¯è½´æœ‰æ•ˆæ©ç 

    """

    lv = _ste_clamp(logv_axes, logv_min, logv_max)

    inv_v = torch.exp(-lv)                 # (B,T,3)

    nll = 0.5 * (e2_axes * inv_v + lv)    # (B,T,3)

    m = mask_axes.float()

    num = (nll * m).sum()

    den = torch.clamp(m.sum(), min=1.0)

    return num / den



def nll_diag_axes_weighted(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,

                           axis_w: torch.Tensor=None,

                           logv_min: float=-16.0, logv_max: float=6.0):

    """

    å„å‘å¼‚æ€§å¯¹è§’é«˜ï¿½?NLLï¼ˆé€è½´ï¿½? æŒ‰è½´æƒé‡ï¿½?

    e2_axes, logv_axes, mask_axes: (B,T,3)

    axis_w: (3,) å½’ä¸€åˆ°å‡ï¿½?1 æ›´ç¨³ï¼ˆå¤–éƒ¨å¯å…ˆåšå½’ä¸€åŒ–ï¼‰

    """

    lv = _ste_clamp(logv_axes, logv_min, logv_max)

    e2_axes = torch.nan_to_num(e2_axes, nan=0.0, posinf=0.0, neginf=0.0)

    e2_axes = torch.where(mask_axes.float() > 0.5, e2_axes, torch.zeros_like(e2_axes))

    inv_v = torch.exp(-lv)                    # (B,T,3)

    nll_axes = 0.5 * (e2_axes * inv_v + lv)  # (B,T,3)

    m = mask_axes.float()

    num = nll_axes.mul(m).sum(dim=(0,1))      # (3,)

    den = m.sum(dim=(0,1)).clamp_min(1.0)     # (3,)

    per_axis = num / den                       # (3,)

    if axis_w is None:

        axis_w = torch.ones_like(per_axis)

    # å½’ä¸€åˆ°å‡ï¿½?1ï¼Œä¾¿ï¿½?lr ç¨³å®š

    axis_w = axis_w * (3.0 / axis_w.sum().clamp_min(1e-6))

    return (per_axis * axis_w).sum(), per_axis.detach()



def nll_studentt_diag_axes(e2_axes: torch.Tensor, logv_axes: torch.Tensor, mask_axes: torch.Tensor,

                           nu: float = 3.0, logv_min: float = -16.0, logv_max: float = 6.0):

    """

    å„å‘å¼‚æ€§å¯¹ï¿½?Student-t NLLï¼ˆé€è½´ï¼‰ã€‚å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥ï¿½?

    e2_axes  : (B,T,3)   æ¯è½´è¯¯å·®å¹³æ–¹

    logv_axes: (B,T,3)   æ¯è½´ log(Ïƒ^2)

    mask_axes: (B,T,3)   æ¯è½´æœ‰æ•ˆæ©ç 

    nu       : è‡ªç”±åº¦å‚æ•°ï¼ˆè¶Šå°è¶Šé‡å°¾ï¼Œè¶Šç¨³å¥ï¼‰

    """

    lv = _ste_clamp(logv_axes, logv_min, logv_max)

    e2_axes = torch.nan_to_num(e2_axes, nan=0.0, posinf=0.0, neginf=0.0)

    e2_axes = torch.where(mask_axes.float() > 0.5, e2_axes, torch.zeros_like(e2_axes))

    v  = torch.exp(lv).clamp_min(1e-12)

    m  = mask_axes.float()

    # Student-t NLLï¼ˆçœç•¥å¸¸æ•°é¡¹ï¼‰ï¼š0.5*log(v) + 0.5*(nu+1)*log(1 + e2/(nu*v))

    nll = 0.5*lv + 0.5*(nu + 1.0) * torch.log1p(e2_axes / (v * nu))

    num = (nll * m).sum()

    den = m.sum().clamp_min(1.0)

    return num / den



def mse_anchor_axes(logv_axes: torch.Tensor, y_var_axes: torch.Tensor, mask_axes: torch.Tensor, lam: float=1e-4) -> torch.Tensor:

    """
    Per-axis log-variance soft anchor: gently pull predicted logv towards log(vendor^2).

    logv_axes   : (B,T,3)

    y_var_axes  : (B,T,3)  â€”ï¿½?é€è½´ vendor æŠ¥å‘Šçš„æ–¹å·®ï¼ˆä¸æ˜¯æ ‡å‡†å·®ï¼‰

    mask_axes   : (B,T,3)

    """

    lv = logv_axes

    y  = torch.clamp(y_var_axes, min=1e-12).log()

    m  = mask_axes.float()

    se = (lv - y)**2 * m

    return lam * se.sum() / torch.clamp(m.sum(), min=1.0)



def adaptive_nll_loss(logv: torch.Tensor, e2: torch.Tensor, mask: torch.Tensor, 

                      use_step_labels, y_anchor: torch.Tensor = None,

                      logv_min: float = -16.0, logv_max: float = 6.0,

                      route: str = "acc") -> torch.Tensor:

    """

    è‡ªé€‚åº”NLLæŸå¤±å‡½æ•°ï¿½?

    - å¦‚æœæœ‰æ­¥çº§æ ‡ç­¾ï¼Œä½¿ç”¨çº¯æ­¥çº§NLL

    - å¦åˆ™ä½¿ç”¨çª—å£é”šç‚¹+è½»ä¸­å¿ƒåŒ–

    

    Args:

        logv: (B,T,D) æ¨¡å‹è¾“å‡ºçš„logæ–¹å·®

        e2: (B,T,D) æ­¥çº§è¯¯å·®å¹³æ–¹ ï¿½?ä»çª—å£æ ‡ç­¾æ‰©å±•çš„è¯¯å·®å¹³æ–¹

        mask: (B,T) æ©ç 

        use_step_labels: bool or tensor æ˜¯å¦ä½¿ç”¨æ­¥çº§æ ‡ç­¾

        y_anchor: (B,D) çª—å£çº§æ ‡ç­¾ï¼ˆä»…åœ¨éæ­¥çº§æ¨¡å¼ä½¿ç”¨ï¼‰

        route: str è·¯ç”±ç±»å‹ ("acc", "gyr")

    """

    # å¤„ç†use_step_labelså¯èƒ½æ˜¯tensorçš„æƒ…ï¿½?

    if isinstance(use_step_labels, torch.Tensor):

        use_step_labels = bool(use_step_labels.item())

    

    # --- æƒ…å†µ Aï¼šæœ‰æ­¥çº§æ ‡ç­¾ï¼ˆæ¨èï¼‰ ---

    if use_step_labels:

        # IMU: ä½¿ç”¨3è½´ISOæŸå¤±

        if e2.size(-1) == 3:

            # å¯¹äºä¸‰è½´æ•°æ®ï¼Œæ±‚å’Œåé™¤ä»¥3

            e2sum = e2.sum(dim=-1)  # (B,T)

            if logv.size(-1) == 3:

                # å¦‚æœæ¨¡å‹è¾“å‡ºä¹Ÿæ˜¯3è½´ï¼Œå–å¹³å‡

                logv_avg = logv.mean(dim=-1)  # (B,T)

            else:

                logv_avg = logv.squeeze(-1)  # (B,T)

            return nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)

        else:

            # å•è½´æ¨¡å¼

            e2sum = e2.squeeze(-1)  # (B,T)

            logv_avg = logv.squeeze(-1)  # (B,T)

            return nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)

    

    # --- æƒ…å†µ Bï¼šåªæœ‰çª—å£æ ‡ç­¾ï¼ˆæ—§EUROC npz çš„æƒå®œä¹‹è®¡ï¼‰ ---

    else:

        # åŸºç¡€æ­¥çº§NLLï¼ˆä½¿ç”¨æ‰©å±•çš„e2ï¼‰

        e2sum = e2.squeeze(-1) if e2.size(-1) == 1 else e2.sum(dim=-1)

        logv_avg = logv.squeeze(-1) if logv.size(-1) == 1 else logv.mean(dim=-1)

        

        # IMU: ä½¿ç”¨3è½´ISOæŸå¤±

        nll = nll_iso3_e2(e2sum, logv_avg, mask, logv_min, logv_max)

        

        # "ä¸­å¿ƒåŒ–"ï¼šè½»å¾®çº¦æŸE[zÂ²] -> 1ï¼Œé˜²æ­¢æ”¶æ•›åˆ°å¸¸æ•°

        sig2 = torch.exp(logv_avg).clamp_min(1e-12)

        z2 = e2sum / sig2  # æ ‡å‡†åŒ–è¯¯å·®

        m = mask.float()

        z2_mean = (z2 * m).sum() / torch.clamp(m.sum(), min=1.0)

        loss_center = (z2_mean - 1.0).pow(2)

        

        # "çª—å£å°ºåº¦é”š"ï¼šç”¨æ—¶é—´å¹³å‡çš„ÏƒÂ²ä¸Y_anchorå¯¹é½

        if y_anchor is not None:

            sig2_win = sig2.mean(dim=1)  # (B,)

            if y_anchor.size(-1) == 3:

                # ä¸‰è½´æ ‡ç­¾æ±‚å¹³å‡

                y_target = y_anchor.mean(dim=-1)  # (B,)

            else:

                y_target = y_anchor.squeeze(-1)  # (B,)

            anchor_loss = F.smooth_l1_loss(sig2_win, y_target)

        else:

            anchor_loss = torch.tensor(0.0, device=logv.device)

        

        return nll + 1e-3 * loss_center + 1e-4 * anchor_loss



def nll_gauss_huber_iso3(e2sum: torch.Tensor, logv: torch.Tensor, mask: torch.Tensor,

                         logv_min: float=-12.0, logv_max: float=6.0,

                         delta: float=1.5,                # Huber é˜ˆï¿½?

                         lam_center: float=5e-2,          # zÂ²å‡å€¼æ ¡å‡†æ­£åˆ™æƒï¿½?

                         z2_target: float=1.0,            # ç›®æ ‡ E[zÂ²]

                         y_anchor: torch.Tensor | None=None,   # (B,) ï¿½?(B,T,1/3)

                         anchor_weight: float=0.0,        # çª—å£å°ºåº¦é”šæƒï¿½?

                         df: float=3.0) -> torch.Tensor:

    """

    Huber-NLLæŸå¤±å‡½æ•°ï¼šå¯¹ z = ï¿½?eÂ²/ÏƒÂ²) ä½¿ç”¨pseudo-Huberï¼Œä¿ç•™å°ºåº¦å­¦ä¹ èƒ½ï¿½?

    

    Args:

        e2sum: (B,T) ï¿½?(B,T,1) è¯¯å·®å¹³æ–¹ï¿½?

        logv: (B,T) ï¿½?(B,T,1) logæ–¹å·®

        mask: (B,T) æ©ç 

        delta: Huberé˜ˆå€¼ï¼Œ|z|â‰¤Î´æ—¶ä¸ºäºŒæ¬¡ï¼Œå¦åˆ™ä¸ºä¸€ï¿½?

        lam_center: zÂ²å‡å€¼æ ¡å‡†æ­£åˆ™æƒï¿½?

        z2_target: ç›®æ ‡E[zÂ²]ï¿½?

        y_anchor: çª—å£å°ºåº¦é”šç‚¹

        anchor_weight: é”šç‚¹æƒé‡

        df: è‡ªç”±åº¦ï¼ˆIMUä¸‰è½´åˆå¹¶ï¿½?3ï¿½?

    

    Returns:

        loss: æ ‡é‡æŸå¤±ï¿½?

    """

    # squeeze ï¿½?(B,T)

    if logv.dim() == 3 and logv.size(-1) == 1: 

        logv = logv.squeeze(-1)

    if e2sum.dim() == 3 and e2sum.size(-1) == 1: 

        e2sum = e2sum.squeeze(-1)

    

    m = mask.float()

    

    # ğŸ”§ å…³é”®ï¼šæ¸…ï¿½?+ åªåœ¨æœ‰æ•ˆæ©ç å†…è®¡å…¥è¯¯ï¿½?

    e2sum = torch.nan_to_num(e2sum, nan=0.0, posinf=0.0, neginf=0.0)

    e2sum = torch.where(m > 0.5, e2sum, torch.zeros_like(e2sum))

    

    # ï¿½?STE clampï¼Œé¿å…æ¢¯åº¦è¢«ç¡¬æˆªï¿½?

    lv = _ste_clamp(logv, logv_min, logv_max)

    v = torch.exp(lv).clamp_min(1e-12)



    # z ï¿½?pseudo-Huber Ï(z)

    z2 = (e2sum / v).clamp_min(0.0)              # (B,T)

    z = torch.sqrt(z2 + 1e-12)

    rho = torch.where(z <= delta, 0.5*z2, delta*(z - 0.5*delta))



    # NLLï¼šï¿½?z) + 0.5 * df * logÏƒÂ²

    nll = (rho + 0.5*df*lv) * m

    den = m.sum().clamp_min(1.0)

    nll = nll.sum() / den



    # è½»åº¦æ ¡å‡†ï¼šæŠŠ E[zÂ²] æ‹‰å› 1ï¼ˆç”¨åŒä¸€ï¿½?denï¿½?

    z2_mean = (z2 * m).sum() / den

    loss_center = (z2_mean - z2_target).pow(2)



    # ï¼ˆå¯é€‰ï¼‰çª—å£å°ºåº¦é”šï¼šæ—¶é—´å¹³å‡ ÏƒÂ² ä¸å¤–éƒ¨é”š y_anchor å¯¹é½

    if (y_anchor is not None) and (anchor_weight > 0.0):

        sig2_win = v.mean(dim=1)                        # (B,)

        if y_anchor.dim() == 3 and y_anchor.size(-1) == 3:

            y_target = y_anchor.mean(dim=(-1,-2))       # (B,T,3)->(B,)

        elif y_anchor.dim() == 2:

            y_target = y_anchor.mean(dim=1)             # (B,)

        else:

            y_target = y_anchor.squeeze(-1)             # (B,)

        anchor_loss = F.smooth_l1_loss(sig2_win, y_target)

    else:

        anchor_loss = torch.zeros((), device=logv.device)



    return nll + lam_center * loss_center + anchor_weight * anchor_loss



