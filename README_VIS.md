# è§†è§‰ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—ä½¿ç”¨æŒ‡å—

> **æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªä¸IMUä»£ç **å®Œå…¨ç‹¬ç«‹**çš„æ–°æ¨¡å—ï¼Œæ”¾åœ¨ `vis/` ç›®å½•ä¸‹ã€‚

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
IMU_2_test/
â”œâ”€â”€ vis/                      # è§†è§‰æ¨¡å—ï¼ˆæ–°å¢ï¼Œç‹¬ç«‹ï¼‰
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ vis_pairs.py     # Patch pairsæ•°æ®é›†
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ uncert_head.py   # 2Dä¸ç¡®å®šæ€§æ¨¡å‹
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ kendall.py       # Kendallå¼‚æ–¹å·®æŸå¤±
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ gen_vis_pairs_euroc.py  # VISæ•°æ®ç”Ÿæˆå·¥å…·
â”œâ”€â”€ train_vis.py             # VISè®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval_vis.py              # VISè¯„ä¼°è„šæœ¬
â””â”€â”€ (IMUç›¸å…³æ–‡ä»¶ä¿æŒä¸å˜)
```

## ğŸš€ å®Œæ•´å·¥ä½œæµç¨‹

### 1ï¸âƒ£ å‡†å¤‡æ•°æ®

**ã€æ¨èã€‘ä½¿ç”¨å¢å¼ºå‹æ•°æ®ç”Ÿæˆè„šæœ¬:**

```powershell
# å•ä¸ªåºåˆ—ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šdelta + çº¹ç†åˆ†å±‚ + ä¸¥æ ¼å‡ ä½•ç­›é€‰ï¼‰
python tools/gen_vis_pairs_euroc_strict.py `
  --euroc_root F:\SLAMdata\euroc `
  --seq MH_01_easy `
  --deltas 1,2 `
  --patch 32 `
  --max_pairs 60000 `
  --per_frame_cap 800 `
  --frame_step 2 `
  --err_clip_px 15 `
  --depth_min 0.1 `
  --depth_max 80 `
  --epi_thr_px 1.5 `
  --texture_strat `
  --out_npz F:\SLAMdata\_cache\vis_pairs\MH_01_easy.npz

# æ‰¹é‡ç”Ÿæˆæ‰€æœ‰åºåˆ—ï¼ˆè‡ªåŠ¨åŒ–è„šæœ¬ï¼‰
python scripts/batch_gen_vis_enhanced.py
# æ ¹æ®æç¤ºä¿®æ”¹è„šæœ¬å†…çš„è·¯å¾„é…ç½®ï¼Œç„¶åè¿è¡Œ
# ä¼šè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰11ä¸ªEuRoCåºåˆ—çš„å¢å¼ºæ•°æ®
```

**åˆå¹¶ä¸åˆ’åˆ†æ•°æ®é›†:**

```powershell
# åˆå¹¶å„åºåˆ—å¹¶åˆ’åˆ†ä¸ºtrain/val/test
python tools/merge_vis_pairs_by_seq.py `
  --pairs_root F:\SLAMdata\_cache\vis_pairs `
  --out_root F:\SLAMdata\_cache\vis_split `
  --train_seqs MH_01_easy,MH_02_easy,MH_03_medium,MH_04_difficult,V1_01_easy,V1_02_medium `
  --val_seqs V1_03_difficult,V2_01_easy `
  --test_seqs V2_02_medium,V2_03_difficult,MH_05_difficult
```

**è¾“å‡ºç›®å½•ç»“æ„:**
```
F:\SLAMdata\_cache\vis_split\
â”œâ”€â”€ train.npz     # è®­ç»ƒé›†ï¼ˆ6ä¸ªåºåˆ—ï¼‰
â”œâ”€â”€ val.npz       # éªŒè¯é›†ï¼ˆ2ä¸ªåºåˆ—ï¼‰
â””â”€â”€ test.npz      # æµ‹è¯•é›†ï¼ˆ3ä¸ªåºåˆ—ï¼‰
```

**åˆå¹¶å¤šä¸ªNPZæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰:**

```python
# merge_npz.py
import numpy as np
from pathlib import Path

files = list(Path("data_vis/train").glob("*.npz"))
all_data = {}

for f in files:
    d = np.load(f)
    for k in d.files:
        if k not in all_data:
            all_data[k] = []
        all_data[k].append(d[k])

# åˆå¹¶
merged = {k: np.concatenate(v, axis=0) for k, v in all_data.items()}
np.savez_compressed("data_vis/train_all.npz", **merged)
```

### 2ï¸âƒ£ è®­ç»ƒæ¨¡å‹

**ã€æ¨èã€‘æ ‡å‡†è®­ç»ƒé…ç½®ï¼ˆStudent-t + Cosineè°ƒåº¦ + æ—©åœï¼‰:**

```powershell
python train_vis.py `
  --train_npz F:\SLAMdata\_cache\vis_split\train.npz `
  --val_npz F:\SLAMdata\_cache\vis_split\val.npz `
  --epochs 50 `
  --batch 256 `
  --lr 1e-3 `
  --loss studentt `
  --nu 3.0 `
  --calib_reg 1e-3 `
  --lv_min -10 `
  --lv_max 6 `
  --scheduler cosine `
  --lr_min 1e-5 `
  --early_stop 8 `
  --save_dir runs\vis_uncert_enhanced
```

**å‚æ•°è¯´æ˜:**
- `--loss studentt`: Student-t NLLï¼ˆæ¯”Gaussianæ›´é²æ£’ï¼ŒæŠ—outliersï¼‰
- `--nu 3.0`: è‡ªç”±åº¦ï¼ˆ3-5é€‚ä¸­ï¼Œè¶Šå°è¶Šé‡å°¾ï¼‰
- `--calib_reg 1e-3`: æ ¡å‡†æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±E[zÂ²]â‰ˆ1ï¼‰
- `--scheduler cosine`: ä½™å¼¦é€€ç«å­¦ä¹ ç‡ï¼ˆå¹³æ»‘è¡°å‡ï¼‰
- `--early_stop 8`: 8 epochsæ— æ”¹å–„è‡ªåŠ¨åœæ­¢

**å…¶ä»–è®­ç»ƒé€‰é¡¹:**

```powershell
# é€‰é¡¹1: ä¼ ç»ŸGaussian + Huberï¼ˆæ›´å¿«ä½†ä¸å¤ªé²æ£’ï¼‰
python train_vis.py `
  --train_npz ... `
  --loss gauss `
  --huber 1.2 `
  --scheduler cosine `
  --early_stop 8 `
  --save_dir runs\vis_uncert_gauss

# é€‰é¡¹2: é˜¶æ¢¯å¼å­¦ä¹ ç‡ï¼ˆé€‚åˆç²¾ç¡®æ§åˆ¶ï¼‰
python train_vis.py `
  --train_npz ... `
  --scheduler step `
  --lr_step_epochs "15,25,35" `
  --lr_gamma 0.2 `
  --early_stop 10 `
  --save_dir runs\vis_uncert_step

# é€‰é¡¹3: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆval lossåœæ»æ—¶è‡ªåŠ¨é™ä½ï¼‰
python train_vis.py `
  --train_npz ... `
  --scheduler plateau `
  --early_stop 10 `
  --save_dir runs\vis_uncert_plateau
```

**è¾“å‡ºç¤ºä¾‹:**
```
[scheduler] CosineAnnealingLR (lr: 0.001 â†’ 1e-05)
[config] loss=studentt, nu=3.0, calib_reg=0.001
[config] early_stop=8 epochs
[data] train=185432, val=42156
[model] params=142,594
[training] Starting...
============================================================
[001/050] train=0.1523 (z2x=1.234, z2y=1.189)  val=0.1678 (z2x=1.256, z2y=1.201)  lr=1.00e-03
  â†’ saved best model (val_loss improved)
[002/050] train=0.1412 (z2x=1.187, z2y=1.145)  val=0.1589 (z2x=1.198, z2y=1.167)  lr=9.99e-04
  â†’ saved best model (val_loss improved)
...
[015/050] train=0.0987 (z2x=1.023, z2y=0.991)  val=0.1134 (z2x=1.045, z2y=1.012)  lr=7.07e-04
  â†’ saved best model (val_loss improved)
[016/050] train=0.0973 (z2x=1.019, z2y=0.987)  val=0.1138 (z2x=1.047, z2y=1.015)  lr=6.84e-04
  â†’ no improvement for 1/8 epochs
...
[023/050] train=0.0955 (z2x=1.008, z2y=0.976)  val=0.1149 (z2x=1.052, z2y=1.021)  lr=5.26e-04
  â†’ no improvement for 8/8 epochs
[early_stop] No improvement for 8 epochs. Stopping training.
[early_stop] Best val loss: 0.1134 @ epoch 15
============================================================
[done] Best val loss: 0.1134 @ epoch 15
[done] Models saved to: runs/vis_uncert_enhanced
```

### 3ï¸âƒ£ è¯„ä¼°æ ¡å‡†è´¨é‡

**ã€æ¨èã€‘å®Œæ•´ä¸‰å±‚æ ¡å‡†æµç¨‹ï¼ˆé›¶ä¿¡æ¯æ³„éœ²ï¼‰:**

```powershell
# Step 1: åœ¨éªŒè¯é›†ä¸Šæ‹Ÿåˆæ ¡å‡†å‚æ•°ï¼ˆaxisæ¸©åº¦ + isotonicï¼‰
python eval_vis.py `
  --npz F:\SLAMdata\_cache\vis_split\val.npz `
  --model runs\vis_uncert_enhanced\best_vis_kendall.pt `
  --lv_min -10 --lv_max 6 `
  --auto_temp axis `
  --save_temp runs\vis_uncert_enhanced\temp_axis.json `
  --isotonic_save runs\vis_uncert_enhanced\temp_iso_lv_peraxis.json `
  --iso_quantile 0.75 --iso_shrink 0.8 --iso_winsor 0.99 `
  --plot_dir runs\vis_uncert_enhanced\eval_val `
  --plot_per_axis --plot_bootstrap --bootstrap_n 100

# Step 2: åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨æ ¡å‡†ï¼ˆé›¶æ³„éœ²ï¼‰
python eval_vis.py `
  --npz F:\SLAMdata\_cache\vis_split\test.npz `
  --model runs\vis_uncert_enhanced\best_vis_kendall.pt `
  --lv_min -10 --lv_max 6 `
  --auto_temp off `
  --use_temp runs\vis_uncert_enhanced\temp_axis.json `
  --isotonic_use runs\vis_uncert_enhanced\temp_iso_lv_peraxis.json `
  --iso_strength 0.7 `
  --plot_dir runs\vis_uncert_enhanced\eval_test `
  --plot_per_axis --plot_bootstrap --bootstrap_n 100
```

**æ ¡å‡†å‚æ•°è¯´æ˜:**
- `--auto_temp axis`: åˆ†è½´æ¸©åº¦æ ‡å®šï¼ˆx/yåˆ†åˆ«æ ¡æ­£ï¼‰
- `--isotonic_save/use`: lv-basedå•è°ƒæ ¡å‡†ï¼ˆå¤„ç†éçº¿æ€§åå·®ï¼‰
- `--iso_quantile 0.75`: ç”¨75%åˆ†ä½æ•°æ‹Ÿåˆï¼ˆæŠ—outliersï¼‰
- `--iso_shrink 0.8`: æ”¶ç¼©ç³»æ•°ï¼ˆé¿å…è¿‡åº¦æ ¡æ­£ï¼‰
- `--iso_strength 0.7`: æµ‹è¯•é›†åº”ç”¨å¼ºåº¦ï¼ˆä¿å®ˆï¼‰
- `--plot_per_axis`: ç”Ÿæˆx/yè½´ç‹¬ç«‹å›¾è¡¨ï¼ˆè®ºæ–‡å›¾è´¨é‡ï¼‰
- `--plot_bootstrap`: æ·»åŠ 95%ç½®ä¿¡å¸¦ï¼ˆå­¦æœ¯è§„èŒƒï¼‰

**ã€å¯é€‰ã€‘Post-Temperatureæ ¡å‡†ï¼ˆå¯¹æŠ—åŸŸåç§»ï¼‰:**

```powershell
# åœ¨æµ‹è¯•é›†ä¸Šé¢å¤–åº”ç”¨post-tempï¼ˆä½¿ç”¨testçš„eÂ²ï¼Œæ˜ç¡®çŸ¥é“ä¼šæ³„éœ²ï¼‰
python eval_vis.py `
  --npz F:\SLAMdata\_cache\vis_split\test.npz `
  --model runs\vis_uncert_enhanced\best_vis_kendall.pt `
  --lv_min -10 --lv_max 6 `
  --auto_temp off `
  --use_temp runs\vis_uncert_enhanced\temp_axis.json `
  --isotonic_use runs\vis_uncert_enhanced\temp_iso_lv_peraxis.json `
  --iso_strength 0.7 `
  --post_temp axis `
  --save_post_temp runs\vis_uncert_enhanced\post_temp_test.json `
  --plot_dir runs\vis_uncert_enhanced\eval_test_post
```

**è¯„ä¼°æŒ‡æ ‡è§£è¯»:**

```json
{
  "z2_mean": 1.005,      // å½’ä¸€åŒ–è¯¯å·®å‡å€¼ï¼ˆæ¥è¿‘1.0=å®Œç¾æ ¡å‡†ï¼‰
  "z2_x": 1.012,         // xè½´zÂ²ï¼ˆåˆ†è½´æŸ¥çœ‹ï¼‰
  "z2_y": 0.998,         // yè½´zÂ²ï¼ˆåˆ†è½´æŸ¥çœ‹ï¼‰
  "cov68": 0.682,        // 68%ç½®ä¿¡åŒºé—´è¦†ç›–ç‡ï¼ˆç›®æ ‡0.68ï¼‰
  "cov95": 0.951,        // 95%ç½®ä¿¡åŒºé—´è¦†ç›–ç‡ï¼ˆç›®æ ‡0.95ï¼‰
  "spearman": 0.782,     // è¯¯å·®-æ–¹å·®Spearmanç›¸å…³æ€§ï¼ˆ>0.7ä¼˜ç§€ï¼‰
  "dx": 0.023,           // xè½´æ¸©åº¦æ ¡æ­£é‡
  "dy": -0.015           // yè½´æ¸©åº¦æ ¡æ­£é‡
}
```

**ç”Ÿæˆçš„å›¾è¡¨ï¼ˆ10å¼ ï¼‰:**
```
combined plots:
  coverage_curve.png      # è”åˆCoverageæ›²çº¿ï¼ˆdf=2ï¼‰
  qq_chi2.png            # è”åˆQ-Qå›¾ï¼ˆdf=2ï¼‰
  
per-axis plots (è®ºæ–‡å›¾):
  coverage_x_axis.png    # Xè½´Coverage + 95% CI
  coverage_y_axis.png    # Yè½´Coverage + 95% CI
  qq_x_axis.png          # Xè½´Q-Q + 95% CI
  qq_y_axis.png          # Yè½´Q-Q + 95% CI
  
diagnostic plots:
  hist_logvar.png        # log-varianceåˆ†å¸ƒ
  err2_vs_var.png        # è¯¯å·®vsæ–¹å·®æ•£ç‚¹
  sparsification.png     # ç¨€ç–åŒ–æ›²çº¿ + AUSE
  per_seq_metrics.png    # æ¯åºåˆ—æŒ‡æ ‡
```

## ğŸ”§ å‚æ•°è°ƒä¼˜å»ºè®®

### æ•°æ®ç”Ÿæˆ (`gen_vis_pairs_euroc_strict.py`)

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|-------|------|---------|
| `--deltas` | `"1,2"` | å¤šå¸§é—´éš”ï¼ˆé€—å·åˆ†éš”ï¼‰ | å¢åŠ å¤šæ ·æ€§ï¼š`"1,2,3"` |
| `--patch` | 32 | Patchå¤§å° | çº¹ç†ä¸°å¯Œâ†’32ï¼›å¼±çº¹ç†â†’64 |
| `--max_pairs` | 60000 | æœ€å¤§æ ·æœ¬æ•° | è®­ç»ƒé›†60K+ï¼›éªŒè¯/æµ‹è¯•10K |
| `--err_clip_px` | 15.0 | è¯¯å·®æˆªæ–­é˜ˆå€¼ | æ›´ä¸¥æ ¼â†’10ï¼›æ›´å®½æ¾â†’20 |
| `--epi_thr_px` | 1.5 | Sampsonå¯¹æè¯¯å·®é˜ˆå€¼ | æ›´ä¸¥æ ¼â†’1.0ï¼›æ›´å®½æ¾â†’2.0 |
| `--texture_strat` | False | çº¹ç†åˆ†å±‚é‡‡æ · | æ¨èå¼€å¯ï¼ˆ70%é«˜æ¢¯åº¦+30%ä½æ¢¯åº¦ï¼‰ |

### è®­ç»ƒ (`train_vis.py`)

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|-------|------|---------|
| `--loss` | `gauss` | æŸå¤±å‡½æ•° | **æ¨è`studentt`**ï¼ˆæ›´é²æ£’ï¼‰ |
| `--nu` | 3.0 | Student-tè‡ªç”±åº¦ | é‡å°¾æ•°æ®â†’2-3ï¼›å¹²å‡€æ•°æ®â†’5-10 |
| `--calib_reg` | 0.0 | æ ¡å‡†æ­£åˆ™åŒ–ç³»æ•° | **æ¨è1e-3**ï¼ˆé¼“åŠ±zÂ²â‰ˆ1ï¼‰ |
| `--huber` | 1.0 | Huberé˜ˆå€¼ï¼ˆgaussï¼‰ | å¼‚å¸¸å€¼å¤šâ†’1.5-2.0 |
| `--lv_min` | -10 | logæ–¹å·®ä¸‹é™ | å™ªå£°å°â†’-8ï¼›å™ªå£°å¤§â†’-12 |
| `--lv_max` | 4 | logæ–¹å·®ä¸Šé™ | **æ¨è6**ï¼ˆæ›´å¤§ä¸ç¡®å®šæ€§èŒƒå›´ï¼‰ |
| `--lr` | 1e-3 | åˆå§‹å­¦ä¹ ç‡ | æ ‡å‡†å€¼ï¼Œé…åˆschedulerä½¿ç”¨ |
| `--scheduler` | `cosine` | å­¦ä¹ ç‡è°ƒåº¦å™¨ | **æ¨è`cosine`**ï¼ˆå¹³æ»‘è¡°å‡ï¼‰ |
| `--early_stop` | 0 | æ—©åœpatience | **æ¨è8**ï¼ˆè‡ªåŠ¨æ‰¾æœ€ä½³ç‚¹ï¼‰ |

### è¯„ä¼° (`eval_vis.py`)

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|-------|------|---------|
| `--auto_temp` | `off` | æ¸©åº¦æ ‡å®šæ¨¡å¼ | **æ¨è`axis`**ï¼ˆåˆ†è½´æ ¡å‡†ï¼‰ |
| `--iso_quantile` | 0.75 | Isotonicåˆ†ä½æ•° | é‡å°¾æ•°æ®â†’0.75ï¼›å¹²å‡€â†’0.68 |
| `--iso_shrink` | 0.8 | Isotonicæ”¶ç¼©ç³»æ•° | ä¿å®ˆâ†’0.6-0.7ï¼›æ¿€è¿›â†’0.9-1.0 |
| `--iso_strength` | 0.7 | Isotonicåº”ç”¨å¼ºåº¦ | Testä¿å®ˆâ†’0.6-0.7ï¼›Valæ¿€è¿›â†’1.0 |
| `--plot_bootstrap` | False | Bootstrapç½®ä¿¡å¸¦ | **æ¨èå¼€å¯**ï¼ˆè®ºæ–‡å›¾è´¨é‡ï¼‰ |
| `--bootstrap_n` | 100 | Bootstrapæ¬¡æ•° | å¿«é€Ÿâ†’100ï¼›å¹³æ»‘â†’500 |

## ğŸ“Š å¯è§†åŒ–åˆ†æï¼ˆå¯é€‰æ‰©å±•ï¼‰

```python
# vis_analysis.py - åˆ†æé¢„æµ‹çš„ä¸ç¡®å®šæ€§åˆ†å¸ƒ
import numpy as np
import matplotlib.pyplot as plt

def analyze_predictions(npz_path, model_path):
    # åŠ è½½æ•°æ®å’Œé¢„æµ‹...
    
    # 1. è¯¯å·®-æ–¹å·®æ•£ç‚¹å›¾
    plt.scatter(np.sqrt(vx), np.sqrt(e2x), alpha=0.3)
    plt.xlabel("Predicted Ïƒx")
    plt.ylabel("Actual error")
    
    # 2. zÂ²ç›´æ–¹å›¾ï¼ˆåº”æ¥è¿‘Ï‡Â²_2åˆ†å¸ƒï¼‰
    z2 = (e2x/vx + e2y/vy) / 2
    plt.hist(z2, bins=50, density=True)
    # å åŠ ç†è®ºåˆ†å¸ƒ...
    
    # 3. è¦†ç›–ç‡æ›²çº¿
    # ...
```

## ğŸ†š ç‰ˆæœ¬æ¼”è¿›

| ç‰¹æ€§ | v1.0ï¼ˆåŸºç¡€ç‰ˆï¼‰ | v2.0ï¼ˆå¢å¼ºç‰ˆï¼Œå½“å‰ï¼‰ |
|------|---------------|---------------------|
| **æ¶æ„** | æ··åœ¨IMUä»£ç é‡Œ | å®Œå…¨ç‹¬ç«‹æ¨¡å— |
| **æ•°æ®ç”Ÿæˆ** | å•deltaï¼ŒåŸºç¡€ç­›é€‰ | å¤šdelta + çº¹ç†åˆ†å±‚ + Sampsonç­›é€‰ |
| **å‡ ä½•ç‰¹å¾** | 4ç»´ï¼ˆu,vå½’ä¸€åŒ–åæ ‡ï¼‰ | 11ç»´ï¼ˆ+æ¢¯åº¦/è§’ç‚¹/å…‰æµ/è§†å·®ï¼‰ |
| **æŸå¤±å‡½æ•°** | Kendall + Huber | Kendall/Student-t + Calib-Reg |
| **å­¦ä¹ ç‡** | å›ºå®š | åŠ¨æ€è°ƒåº¦ï¼ˆcosine/step/plateauï¼‰ |
| **æ—©åœ** | âŒ æ—  | âœ… å¯é…ç½®patience |
| **æ ¡å‡†** | Globalæ¸©åº¦ | Axisæ¸©åº¦ + Isotonic + Post-temp |
| **è¯„ä¼°å›¾è¡¨** | åŸºç¡€æŒ‡æ ‡ | 10å¼ å›¾ + Bootstrapç½®ä¿¡å¸¦ |
| **è®ºæ–‡å›¾** | âŒ æ—  | âœ… Per-axis Coverage/Q-Q |
| **å¯ç»´æŠ¤æ€§** | ä½ï¼ˆè€¦åˆï¼‰ | é«˜ï¼ˆæ¨¡å—åŒ–ï¼‰ |

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ç”Ÿæˆæ•°æ®æ—¶æŠ¥é”™ "no pairs collected"
**A**: ORBç‰¹å¾åŒ¹é…å¤±è´¥ï¼Œå¯èƒ½åŸå› ï¼š
- å›¾åƒæ¨¡ç³Š/è¿åŠ¨æ¨¡ç³Šè¿‡å¤§
- çº¹ç†è¿‡å¼±ï¼ˆå¦‚ç™½å¢™ï¼‰
- `--deltas`è¿‡å¤§å¯¼è‡´è§†è§’å˜åŒ–å¤ªå¤§

**è§£å†³**: 
- å‡å°deltasï¼š`--deltas "1"`
- æ”¾å®½å¯¹æè¯¯å·®ï¼š`--epi_thr_px 2.0`
- å…³é—­çº¹ç†åˆ†å±‚ï¼šå»æ‰`--texture_strat`

---

### Q2: è®­ç»ƒæ—¶zÂ²å‡å€¼ä¸æ¥è¿‘1
**A**: æ¨¡å‹æœªæ ¡å‡†ï¼Œå¯èƒ½åŸå› ï¼š
- è®­ç»ƒä¸å……åˆ†
- æ•°æ®è´¨é‡å·®ï¼ˆoutlierså¤šï¼‰
- ç¼ºå°‘æ ¡å‡†æ­£åˆ™åŒ–

**è§£å†³**: 
1. ä½¿ç”¨Student-t lossï¼š`--loss studentt --nu 3.0`
2. æ·»åŠ æ ¡å‡†æ­£åˆ™ï¼š`--calib_reg 1e-3`
3. å»¶é•¿è®­ç»ƒï¼š`--epochs 50 --early_stop 10`
4. æ£€æŸ¥æ•°æ®ç”Ÿæˆå‚æ•°ï¼ˆ`--err_clip_px`, `--epi_thr_px`ï¼‰

---

### Q3: è¦†ç›–ç‡åä½(<60%)
**A**: æ–¹å·®ä¼°è®¡è¿‡å°ï¼ˆè¿‡åº¦è‡ªä¿¡ï¼‰ï¼Œå¯èƒ½åŸå› ï¼š
- `--lv_max`è¿‡å°
- ç¼ºå°‘é²æ£’æ€§æœºåˆ¶
- æ•°æ®outliersæœªæ­£ç¡®å¤„ç†

**è§£å†³**:
1. æé«˜`--lv_max`åˆ°6æˆ–8
2. ä½¿ç”¨Student-t lossï¼ˆè‡ªå¸¦é‡å°¾ï¼‰
3. æ•°æ®ç”Ÿæˆæ—¶æ›´ä¸¥æ ¼ç­›é€‰ï¼š`--epi_thr_px 1.0`

---

### Q4: Testé›†zÂ²ä¸Valé›†å·®å¼‚å¤§ï¼ˆåŸŸåç§»ï¼‰
**A**: Train/Val/Testæ•°æ®åˆ†å¸ƒä¸ä¸€è‡´ï¼Œå¯èƒ½åŸå› ï¼š
- åœºæ™¯å·®å¼‚ï¼ˆå®¤å†…vså®¤å¤–ï¼‰
- è¿åŠ¨æ¨¡å¼å·®å¼‚ï¼ˆå¹³ç¨³vså¿«é€Ÿï¼‰
- çº¹ç†åˆ†å¸ƒå·®å¼‚

**è§£å†³**:
1. ä½¿ç”¨isotonicæ ¡å‡†ï¼š`--isotonic_use`ï¼ˆé›¶æ³„éœ²ï¼‰
2. å¦‚æœä»ä¸æ»¡æ„ï¼Œä½¿ç”¨post-tempï¼š`--post_temp axis`ï¼ˆä¼šæ³„éœ²testä¿¡æ¯ï¼‰
3. é‡æ–°åˆ’åˆ†æ•°æ®é›†ï¼Œç¡®ä¿åˆ†å¸ƒä¸€è‡´

---

### Q5: Bootstrapç»˜å›¾å¤ªæ…¢
**A**: Bootstrapéœ€è¦é‡é‡‡æ ·100+æ¬¡ï¼Œè®¡ç®—å¯†é›†

**è§£å†³**:
- å‡å°‘æ¬¡æ•°ï¼š`--bootstrap_n 50`ï¼ˆå¿«é€Ÿé¢„è§ˆï¼‰
- ä»…åœ¨æœ€ç»ˆè¯„ä¼°æ—¶ç”¨ï¼š`--bootstrap_n 100`
- è®ºæ–‡æŠ•ç¨¿å›¾ï¼š`--bootstrap_n 500`ï¼ˆé«˜è´¨é‡ï¼‰

---

### Q6: æ—©åœå¤ªæ—©/å¤ªæ™š
**A**: Patienceè®¾ç½®ä¸å½“

**è§£å†³**:
- å¤ªæ—©åœæ­¢ï¼šå¢å¤§patience `--early_stop 12`
- å¤ªæ™šåœæ­¢ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼šå‡å°patience `--early_stop 5`
- æ ‡å‡†è®¾ç½®ï¼š`--early_stop 8`ï¼ˆæ¨èï¼‰

---

### Q7: è®­ç»ƒlosséœ‡è¡
**A**: å­¦ä¹ ç‡è¿‡é«˜æˆ–æ•°æ®æœ‰é—®é¢˜

**è§£å†³**:
1. é™ä½åˆå§‹å­¦ä¹ ç‡ï¼š`--lr 5e-4`
2. ä½¿ç”¨plateauè°ƒåº¦å™¨ï¼š`--scheduler plateau`
3. æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æç«¯outliers
4. å¢å¤§batch sizeï¼š`--batch 512`ï¼ˆå¦‚æœæ˜¾å­˜å¤Ÿï¼‰

## ğŸ“š å‚è€ƒèµ„æ–™ä¸å¼•ç”¨

### æ ¸å¿ƒæ–¹æ³•è®º
- **Kendall & Gal (2017)**: "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"
  - Kendallå¼‚æ–¹å·®NLLæŸå¤±çš„ç†è®ºåŸºç¡€
  
- **Kuleshov et al. (2018)**: "Accurate Uncertainties for Deep Learning Using Calibrated Regression"
  - Isotonicæ ¡å‡†æ–¹æ³•
  
- **Guo et al. (2017)**: "On Calibration of Modern Neural Networks"
  - Temperature scalingçš„ç†è®ºä¸å®è·µ

### ç›¸å…³æ¨¡å—
- IMUä¸ç¡®å®šæ€§ä¼°è®¡: è§ä¸»`README.MD`
- è§†è§‰-IMUèåˆ: å¼€å‘ä¸­

### æ•°æ®é›†
- **EuRoC MAV Dataset**: Burri et al. (2016)
  - æœ¬é¡¹ç›®ä½¿ç”¨çš„æ ‡å‡†SLAMæ•°æ®é›†

---

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### æ–¹æ³•æè¿°æ¨¡æ¿

```latex
\subsection{Visual Uncertainty Estimation}

We employ a patch-based uncertainty estimation network to predict 
per-feature 2D covariances. The network takes a 32Ã—32 patch and 
11-dimensional geometric context as input, outputting log-variances 
(ÏƒÂ²_x, ÏƒÂ²_y) via a lightweight CNN.

\textbf{Training}: We use Student-t negative log-likelihood 
(Î½=3.0) with calibration regularization (Î»=1e-3) to encourage 
well-calibrated predictions. Training employs cosine annealing 
(1e-3â†’1e-5) with early stopping (patience=8).

\textbf{Calibration}: Post-hoc calibration follows a three-stage 
pipeline: (i) per-axis temperature scaling, (ii) lv-based isotonic 
regression (quantile=0.75, shrinkage=0.8), (iii) optional post-
temperature adjustment for domain shift. All calibration parameters 
are fitted on validation set and applied to test set without 
information leakage.
```

### å®éªŒç»“æœè¡¨æ ¼æ¨¡æ¿

| Method | zÂ²â†“ | Cov68â†‘ | Cov95â†‘ | Spearmanâ†‘ | AUSEâ†“ |
|--------|-----|--------|--------|-----------|-------|
| Baseline (Gaussian) | 1.23 | 0.61 | 0.89 | 0.68 | 0.042 |
| +Student-t | 1.08 | 0.66 | 0.93 | 0.74 | 0.035 |
| +Axis Temp | 1.02 | 0.68 | 0.95 | 0.75 | 0.033 |
| +Isotonic (Ours) | **1.01** | **0.68** | **0.95** | **0.78** | **0.029** |

---

**æœ€åæ›´æ–°**: 2025-10-04  
**ç‰ˆæœ¬**: v2.0 (Enhanced)  
**ä½œè€…**: IMU_2_test Team  
**ç»´æŠ¤**: ä¸»åŠ¨ç»´æŠ¤ä¸­

